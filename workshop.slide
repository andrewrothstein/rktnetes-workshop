Basics with rkt, the container engine by CoreOS

27th June 2016

Sergiusz Urbaniak
rkt Engineer, CoreOS
sur@coreos.com
@_surbaniak

* whoami

Software Engineer at CoreOS

rkt developer

Yes ... I do use the Linux Desktop :-)

* Overview

.image rkt8s.png _ 600

- Learn about rkt
- Use rkt
- Learn about Kubernetes
- Use Kubernetes+rkt = rktnetes

Requirements:

- Vagrant
- Virtualbox

* Setup

  git clone https://github.com/coreos/rktnetes-workshop
  cd vagrant
  vagrant up
  vagrans ssh

* Starting nginx

  sudo rkt run --insecure-options=image docker://nginx

In another terminal:

  $ rkt l
  [vagrant@localhost ~]$ rkt l
  UUID		APP      ... NETWORKS
  0e32f69d	busybox  ... default:ip4=172.16.28.2

  $ curl 172.16.28.2
  curl 172.16.28.2
  <!DOCTYPE html>
  <html>
  <head>
  <title>Welcome to nginx!</title>
  ...

quit by hitting `Ctrl-]` three times

* Starting an interactive busybox

  $ sudo rkt run --insecure-options=image --interactive docker://progrium/busybox
  
  image: using image from local store for image name coreos.com/rkt/stage1-coreos:1.17.0
  image: using image from local store for url docker://progrium/busybox
  networking: loading networks from /etc/rkt/net.d
  networking: loading network default with type ptp
  / #

 

  / # ping www.google.de
  ping: bad address 'www.google.de'

Note: pod doesn't have DNS by default

Start the pod above using `--dns=8.8.8.8`

* Let's step back

.image os-procs.png _ 200

In a classical "OS" setup we have:

- A supervisor, aka "init daemon", aka PID1
- Not only one process, but many processes
- Processes work together, either via localhost net, IPC
- Communicate with outside world

* rkt - Pods

.image pod-apps.png _ 300

- Grouping of applications executing in a shared context (network, namespaces, volumes)
- Shared fate
- The _only_ execution primitive: single applications are modelled as singleton pods

* rkt - Sample Pod: micro-service talking to Redis

.image redis-service.png _ 230

  sudo rkt run \
    --insecure-options=image \
    docker://redis \
    s-urbaniak.github.io/images/redisservice:0.0.2

.link https://github.com/s-urbaniak/redis-service

* Pods - Patterns, patterns everywhere

Container/App Design Patterns

- Kubernetes enables new design patterns
- Similar to OO patterns
- Key difference: technologically agnostic

.link http://blog.kubernetes.io/2016/06/container-design-patterns.html
.link https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf

* Pods - Sidecar pattern

.image pattern-sidecar.png _ 400

- Auxiliary app
- Extend, enhance main app

Pros:

- Separate packaging units
- Each app contained in a separate failure boundary
- Potentially different technologies/languages

* Pods - Ambassador pattern

.image pattern-ambassador.png _ 400

- Proxy communication
- Separation of concerns
- Main app has simplified view

* Pods - Adapter pattern

.image pattern-adapter.png _ 400

- Use an interface of an existing app as another interface
- Very useful for legacy apps, translating protocols

* Pods - Leader election pattern

.image pattern-leader.png _ 400

- Separate the leader logic from the election logic
- Swappable algorithms/technologies/environments

Ready-to-use generic leader elector:

.link http://blog.kubernetes.io/2016/01/simple-leader-election-with-Kubernetes.html

* Pods - Work queue pattern

.image pattern-work-queue.png _ 400

- Separate app logic from queue enqueing/dequeing

* Pods - Scatter gather pattern

.image pattern-scatter-gather.png _ 400

- Main app sends a simple request
- Auxiliary app implements complex scatter/gather logic
- Fan-Out/Fan-In requests separate from main app

* Building pods

.link https://github.com/s-urbaniak/inspector
.link https://coreos.com/rkt/docs/latest/signing-and-verification-guide.html

- Build a small Go application
- Build an ACI image
- Sign the image using `gpg`

* Signing builds (locally)

1. Create a key (if you don't have one already)

  $ gpg2 --full-gen-key

2. Trust the key

  $ gpg2 --armor --export your@email.com >public.asc
  $ sudo rkt trust --prefix=your/image/name ./public.asc

* rkt - Networking

The CNI (Container Network Interface)

.image pod-net.png _ 300

- Abstraction layer for network configuration
- Single API to multiple, extensible networks
- Narrow, simple API
- Plugins for third-party implementations

* rkt - Networking - Host Mode

.image host-mode.png _ 300

  rkt run --net=host ...

- Inherit the network namespace of the process that is invoking rkt.
- Pod apps are able to access everything associated with the hostâ€™s network interfaces.

*Workshop*time*

1. Start nginx using `--net=host`

* rkt - Networking - Default Mode (CNI ptp)

.image ptp.png _ 300

  rkt run --net ...
  rkt run --net=default ...

.link https://github.com/containernetworking/cni/blob/master/Documentation/ptp.md

- Creates a virtual ethernet pair
- One placed in the pod
- Other one placed on the host

* rkt - Networking - CNI brigde

.image bridge.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/bridge.md

- Creates a virtual ethernet pair
- One placed in the pod
- Other one placed on the host
- Host veth pluggind into a linux bridge

* rkt - Networking - CNI macvlan

.image macvlan.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/macvlan.md

- Functions like a switch
- Pods get different MAC addresses
- Pods share the same physical device

* rkt - Networking - CNI ipvlan

.image ipvlan.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/ipvlan.md

- Functions like a switch
- Pods share the same MAC address
- Pods get different IPs
- Pods share the same physical device

* rkt - Networking - SDN (software defined networking)

.image pod-net-canal.png 300 _

- Communicate with pods across different _hosts_
- Each pod across all hosts gets its own IP
- Virtual overlay network

* rkt - Networking

Example: bridge two pods, so they can see each other

  $ cat /etc/rkt/net.d/bridge.conf
  {
      "name": "bridge-nat",
      "type": "bridge",
      "bridge": "rkt-bridge-nat",
      "ipMasq": true,
      "isGateway": true,
      "ipam": {
          "type": "host-local",
          "subnet": "10.2.0.0/24",
          "routes": [
                  { "dst": "0.0.0.0/0" }
          ]
      }
  }
 
  $ sudo rkt run --net=bridge docker://nginx
 
  $ sudo rkt run --net=bridge --interactive docker://progrium/busybox
  $ wget 10.2.0.2

* Kubernetes - Overview

.image flower.png

.link https://github.com/kubernetes/kubernetes

- Open Source project initiated by Google
- Cluster-level container orchestration

Handles:

- Scheduling/Upgrades
- Failure recovery
- Scaling

* k8s - Components - API Server

- Validates, configures, persists for all Kubernetes API objects
- Provides REST based operations via JSON
- Uses etcd is its database

* k8s - Components - Controller Manager

.image control-loop.png _ 300

- Embeds the core control loops
- Is _state-less_
- Is decoupled from etcd via the API-server

Just a daemon talking to etcd

* k8s - Components - Scheduler

Schedules pods on nodes

- Policy-rich
- Topology-aware
- Workload-specific
- Considers resource requirements, QoS, HW/SW/policy constraints, ...

Again ... just a daemon talking to etcd

* k8s - Components - Kubelet

- Primary node agent
- Starts/Stops/Supervises pods on its node

Sigh ... just a daemon talking to etcd and to *rkt* !!!

Instructs rkt to:

- Fetch pods
- Start, and stop pods
- Exec into pods
- Tail pod logs

PS: We are working very hard to make *rkt* a first class runtime in Kubernetes.

* k8s - Components - Kube Proxy

Primary network proxy on each node.

- configures `iptables` to reflect services
- Forwards TCP/UDP streams across a set of backends

* k8s - The big picture

.image k8s-overview.png 350 _

1. Watches created pods, assigns them to nodes
2. Runs controllers (node ctl, replication ctl, endpoint ctl, service account ctl, ...)
3. Watches for pods assigned to its node, runs *rkt*!
4. Manipulates network rules (iptables) for services on a node, does connection forwarding.

* k8s - Let's do it!

*Workshop*time*

1. Start Kubernetes
2. Create a service
3. Launch nginx
4. Launch busybox
5. Exec into busybox
6. Launch the Kubernetes dashboard
7. Open it from the host machine

See the next slides for templates.

* rktnetes - nginx Service

*Workshop*time*

  kind: Service
  apiVersion: v1
  metadata:
    labels:
      app: nginx
    name: nginx
  spec:
    type: NodePort
    ports:
    - port: 80
      targetPort: 80
    selector:
      app: nginx

 

  kubectl create -f nginx-svc.yaml

* rktnetes - nginx Replication Controller

    kind: ReplicationController
    apiVersion: v1
    metadata:
      labels:
        app: nginx
      name: nginx
    spec:
      replicas: 1
      selector:
        app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80
              protocol: TCP

 

  kubectl create -f nginx-rc.yaml

* rktnetes - busybox pod

*Workshop*time*

  apiVersion: v1
  kind: Pod
  metadata:
    name: busybox
  spec:
    containers:
    - name: busybox
      image: progrium/busybox
      args:
      - sleep
      - "1000000"

 

  kubectl create -f busybox.yaml

Exec into the pod:

  kubectl exec -ti busybox /bin/sh

* rktnetes - Dashboard

Deploy the Kubernetes Dashboard

*Workshop*time*

  kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
  ip address show dev eth1
  kubectl get svc --namespace=kube-system kubernetes-dashboard -o yaml

1. Get the assigned NodePort
2. Open http://<vm-ip>:NodePort on the host

* rktnetes - Ingress

Deploy an Ingress Controller based on traefik

*Workshop*time*

  kubectl create -f /vagrant/traefik.yaml

Add the following entry to `/etc/hosts`, replace `vm-ip` with the VM IP

  <vm-ip>	traefik.rktnetes

.link http://traefik.rktnetes http://traefik.rktnetes

* rktnetes - Ingress

.image traefik-ingress.png _ 1000

* rktnetes - Expose Dashboard via Ingress

*Workshop*time*

  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
   name: kubernetes-dashboard-ingress
   namespace: kube-system
  spec:
   rules:
     - host: dashboard.rktnetes
       http:
         paths:
           - path: /
             backend:
               serviceName: kubernetes-dashboard
               servicePort: 80

Add the following entry to `/etc/hosts`, replace `vm-ip` with the VM IP

  <vm-ip>	dashboard.rktnetes

.link http://dashboard.rktnetes http://dashboard.rktnetes

* rktnetes - Expose Dashboard via Ingress

.image dashboard-ingress.png _ 1000
